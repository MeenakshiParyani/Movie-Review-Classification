{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import spatial\n",
    "\n",
    "import matplotlib.pyplot as plt # side-stepping mpl backend\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "# from sklearn import cross_validation\n",
    "import heapq\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "trainingData  = pd.read_csv('data/train.dat.txt', sep=\"\\t\", encoding='utf-8', header=None, names=[\"rating\",\"review\"])[:2];\n",
    "testData = pd.read_csv('data/test.dat.txt', sep=\"\\t\", encoding='utf-8', header=None, names=[\"review\"])[:2];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning\n",
      "   rating                                             review\n",
      "0      -1  Although a film with Bruce Willis is always wo...\n",
      "After Cleaning\n",
      "[[u'although', u'film', u'bruce', u'willis', u'always', u'worth', u'watching', u'better', u'skip', u'watched', u'television', u'plunk', u'cash', u'lucky', u'plot', u'develops', u'slowly', u'slowly', u'although', u'first', u'minutes', u'quite', u'believable', u'gets', u'unbelievable', u'towards', u'highly', u'questionable', u'seasoned', u'soldier', u'like', u'waters', u'would', u'disobey', u'direct', u'orders', u'even', u'would', u'rest', u'platoon', u'would', u'know', u'puts', u'direct', u'danger', u'know', u'certainly', u'follow', u'heck', u'says', u'despite', u'direct', u'orders', u'remember', u'still', u'nice', u'scenes', u'movie', u'somewhat', u'save', u'village', u'total', u'population', u'massacred', u'rebels', u'well', u'save', u'dozen', u'villagers', u'rest', u'already', u'killed', u'strange', u'part', u'take', u'trucks', u'rebels', u'left', u'behind', u'rather', u'foot', u'maybe', u'roads', u'unsafe', u'explanation', u'anyway', u'think', u'earned', u'movie', u'point', u'gave', u'made', u'movie', u'insult', u'brain', u'hence', u'completely', u'unbelievable', u'group', u'soldiers', u'kill', u'many', u'rebels', u'without', u'hurt', u'killed', u'near', u'loose', u'comrades', u'fight', u'army', u'nearly', u'believe', u'fight', u'army', u'many', u'kill', u'hundreds', u'loose', u'rounds', u'round', u'ammo', u'never', u'grenades', u'claymore', u'mines', u'machine', u'even', u'stuff', u'carrying', u'around', u'even', u'laptop', u'shows', u'activity', u'enemy', u'rebels', u'laptop', u'battery', u'goes', u'days', u'really', u'think', u'crap.', u'guess', u'turn', u'brain', u'completely', u'accept', u'rebels', u'bunch', u'idiots', u'give', u'movie', u'high', u'rating', u'skip', u'saves', u'time'], [u'movie', u'slower', u'molasses', u'january', u'alaska', u'togeather', u'preview', u'award', u'managing', u'every', u'seconds', u'interisting', u'preview', u'wake', u'people', u'watching', u'several', u'times', u'felt', u'woken', u'film', u'taken', u'hoping', u'something', u'actually', u'happen', u'nothing', u'ever', u'easy', u'loose', u'track', u'people', u'motives', u'characters', u'flat', u'uninteristing', u'movie', u'hoped', u'everyone', u'would', u'died', u'everyone', u'runs', u'around', u'either', u'contemptible', u'petty', u'pitiful', u'usually', u'three', u'worse', u'watched', u'minute', u'added', u'features', u'kicks', u'giggles', u'understand', u'people', u'smug', u'socially', u'aware', u'spend', u'time', u'movie', u'patting', u'back', u'might', u'worth', u'watching', u'brought', u'expecting', u'excitement', u'lecture', u'social', u'awareness', u'blery', u'eyes', u'sandman']]\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print('Before Cleaning');\n",
    "print (trainingData[0:1])\n",
    "\n",
    "def preProcess(reviews):\n",
    "#     print(reviews);\n",
    "    processedReviews = [];\n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review);\n",
    "        filteredTokens = [];\n",
    "        for token in list(tokens):\n",
    "#             print(token);\n",
    "            # if it is a stopword then eliminate\n",
    "            if token.lower() in stopwords.words('english'):\n",
    "#                 print('removing stopword ' + token);\n",
    "                continue;\n",
    "            # if it is punctuation then eliminate\n",
    "            if token.lower() in set(string.punctuation):\n",
    "#                 print('removing punct ' + token);\n",
    "                continue;\n",
    "            if len(token)<=3:\n",
    "#                 print('removing small ' + token);\n",
    "                continue;\n",
    "            token = token.lower();\n",
    "            filteredTokens.append(token);\n",
    "#         print(filteredTokens);\n",
    "        processedReviews.append(filteredTokens);\n",
    "#     print(len(processedReviews));\n",
    "    return processedReviews;\n",
    "print('After Cleaning')\n",
    "XTrain = preProcess(trainingData['review']);\n",
    "print(XTrain);\n",
    "print (len(XTrain));\n",
    "XTest = preProcess(testData['review']);\n",
    "print (len(XTest));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 191.\n",
      "Number of unique words: 152.\n",
      "191\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "# count frequencies for all words in the Training data\n",
    "def countFrequency(data):\n",
    "    wordCountInData = Counter()\n",
    "    for d in data:\n",
    "    #     print(d);\n",
    "        for w in d:\n",
    "    #         print(w);\n",
    "            if w not in wordCountInData:\n",
    "                wordCountInData[w]=1\n",
    "            else:\n",
    "                wordCountInData[w] += 1\n",
    "    print(\"Number of unique words: %d.\" % len(wordCountInData));\n",
    "    return wordCountInData;\n",
    "\n",
    "wordCountInTrainingData = countFrequency(XTrain);\n",
    "wordCountInTestData = countFrequency(XTest);\n",
    "print(len(wordCountInTrainingData));\n",
    "print(len(wordCountInTestData));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "#Top percent of training data\n",
    "topPct = 1;\n",
    "topPctOfTraining = wordCountInTrainingData.most_common(int(round(len(wordCountInTrainingData)*topPct)));\n",
    "topPctOfTest = wordCountInTestData.most_common(int(round(len(wordCountInTestData)*topPct)));\n",
    "\n",
    "# Get a set of topPctOfTraining\n",
    "topPctOfTrainingSet = set()\n",
    "for word in topPctOfTraining:\n",
    "    topPctOfTrainingSet.add(word[0]);\n",
    "\n",
    "# Get a set of topPctOfTest\n",
    "topPctOfTestSet = set()\n",
    "for word in topPctOfTest:\n",
    "    topPctOfTestSet.add(word[0]);\n",
    "    \n",
    "# print(topPctOfTrainingSet);\n",
    "# print(topPctOfTestSet);\n",
    "print(len(topPctOfTrainingSet));\n",
    "print(len(topPctOfTestSet));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words common in both test and train: 17.\n"
     ]
    }
   ],
   "source": [
    "testTrainSet =  set.intersection(topPctOfTrainingSet,topPctOfTestSet);\n",
    "print(\"Number of unique words common in both test and train: %d.\" % len(testTrainSet));\n",
    "# print(testTrainSet);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "cnt = Counter(testTrainSet);\n",
    "top_percentile = 1.0\n",
    "features = cnt.most_common(int(round(len(cnt)*top_percentile)))\n",
    "featuresCount = len(features)\n",
    "# print(features);\n",
    "print(featuresCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "[u'even', u'made', u'point', u'movie', u'loose', u'scenes', u'first', u'times', u'every', u'take', u'characters', u'time', u'nothing', u'still', u'think', u'film', u'nice']\n"
     ]
    }
   ],
   "source": [
    "featureList= [];\n",
    "for feature in features:\n",
    "    featureList.append(feature[0])\n",
    "print(len(featureList));\n",
    "print(featureList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Buid CSR matrix\n",
    "\n",
    "def build_matrix1(mat):\n",
    "    scipy.sparse.csr_matrix(mat)\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    dim = len(featureList)\n",
    "    feature_set = set(featureList[:dim])\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "#         print('d---')\n",
    "#         print(d)\n",
    "#         set_d = set(d)\n",
    "        set_d = set()\n",
    "        for word in d:\n",
    "#             print(word)\n",
    "            set_d.add(word);\n",
    "        print('set d--')\n",
    "        print(set_d);\n",
    "        print('features--')\n",
    "        print(feature_set);\n",
    "        d = list(set.intersection(feature_set,set_d))\n",
    "        print('common elemets %d',len(d));\n",
    "        print('com')\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            print('adding word ', w) ;\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "    print(idx);\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        set_d = set(d)\n",
    "        d = list(set.intersection(feature_set,set_d))\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set d--\n",
      "set([u'rating', u'What', u'Who', u'rebels', u'being', u'skip', u'laptop', u'saves', u'Lucky', u'rest', u'gun', u'brain', u'kill', u'go', u'follow', u'bunch', u'certainly', u'group', u'earned', u'(', u'point', u'claymore', u',', u'killed', u'better', u'to', u'only', u'Carrying', u'Although', u'save', u'villagers', u'worth', u'gave', u'it.', u'do', u'them', u'his', u'which', u'watching', u'very', u'know', u'they', u'despite', u'foot', u'highly', u'Where', u'him', u'nearly', u'enemy', u'like', u'trucks', u'did', u'die', u'Anyway', u'platoon', u'always', u'where', u'round', u'population', u'because', u'says', u'me.', u'some', u'direct', u'around', u'scenes', u'insult', u'Only', u'are', u'soldiers', u'our', u'Lt.', u'out', u'even', u\"n't\", u'what', u'give', u'for', u'plunk', u'Bruce', u'movie', u'mines', u'/', u'your', u'behind', u'stuff', u'run', u'7', u'got', u'get', u'?', u'shows', u'fight', u'dozen', u'loose', u'quite', u'completely', u'not', u'let', u'br', u'hundreds', u'by', u'on', u'puts', u'of', u'30', u'has', u'days', u'soldier', u'heck', u'Grenades', u'hence', u'Still', u'or', u'ammo', u'first', u'already', u'danger', u'battery', u'Willis', u'If', u'one', u'down', u'village', u'roads', u'total', u'rounds', u'plot', u'guess', u'would', u'army', u'Waters', u'there', u'idiots', u'.', u'few', u'slowly', u'goes', u'seasoned', u'themselves', u'was', u'500', u'>', u'questionable', u'that', u'explanation', u'crap.', u'Well', u'but', u'hurt', u'part', u'somewhat', u'turn', u'off', u'believe', u'with', u'he', u'And', u'television', u'made', u'this', u'cash', u'<', u'will', u'near', u'can', u'many', u'more', u'minutes', u'and', u'up', u'is', u'in', u'unsafe', u'it', u'accept', u'an', u'high', u'Lt', u'comrades', u'have', u'watched', u'believable', u'activity', u'orders', u'film', u'if', u'end', u'no', u')', u'Never', u'machine', u'strange', u'take', u'They', u'you', u'gets', u'nice', u'M60', u'unbelievable', u'towards', u\"'s\", u'I', u'Maybe', u'time', u'Can', u'develops', u'The', u'the', u'a', u'massacred', u'remember', u'It', u'think', u'rather', u'without', u'so', u'RPG', u'disobey', u'Really', u'left'])\n",
      "features--\n",
      "set([u'even', u'made', u'nice', u'point', u'movie', u'loose', u'scenes', u'times', u'every', u'take', u'characters', u'time', u'nothing', u'still', u'think', u'film', u'first'])\n",
      "('common elemets %d', 12)\n",
      "com\n",
      "('adding word ', u'even')\n",
      "('adding word ', u'made')\n",
      "('adding word ', u'point')\n",
      "('adding word ', u'movie')\n",
      "('adding word ', u'loose')\n",
      "('adding word ', u'scenes')\n",
      "('adding word ', u'take')\n",
      "('adding word ', u'time')\n",
      "('adding word ', u'first')\n",
      "('adding word ', u'think')\n",
      "('adding word ', u'film')\n",
      "('adding word ', u'nice')\n",
      "set d--\n",
      "set([u'giggles', u'all', u'just', u'being', u'over', u'through', u'had', u',', u'actually', u'to', u'does', u'blery', u'easy', u'pitiful', u'might', u'By', u'awareness', u'...', u'them', u'woken', u'watching', u'socially', u'Alaska', u'every', u'they', u'lecture', u'smug', u'minute', u'runs', u'contemptible', u'did', u'togeather', u'bad', u'uninteristing', u'either', u'two', u'everyone', u'slower', u'people', u'back', u'are', u'happen', u'excitement', u'for', u'managing', u'movie', u'/', u'seconds', u'Molasses', u'got', u'get', u'ever', u'we', u'who', u'This', u'loose', u'worse', u'br', u'put', u'on', u'about', u'of', u'worth', u'30', u'times', u'expecting', u'social', u'usually', u'or', u'features', u'into', u'one', u'brought', u'should', u'petty', u'would', u'three', u'been', u'.', u'Most', u'motives', u'taken', u'themselves', u'preview', u'was', u'>', u'flat', u'eyes', u'that', u'January', u'award', u'hoped', u'wake', u'kicks', u'with', u'And', u'up', u'<', u'will', u'were', u'and', u'hoping', u'then', u'is', u'it', u'an', u'patting', u'something', u'have', u'in', u'watched', u'saw', u'film', u'around', u'end', u'After', u'interisting', u'how', u\"'24\", u'you', u'several', u'added', u\"'s\", u'track', u'felt', u'sandman', u'but', u'characters', u'nothing', u'The', u'aware', u'died', u'man', u'a', u'Everyone', u'I', u\"'\", u'It', u'spend', u'time', u'understand', u'the', u'having', u'If'])\n",
      "features--\n",
      "set([u'even', u'made', u'nice', u'point', u'movie', u'loose', u'scenes', u'times', u'every', u'take', u'characters', u'time', u'nothing', u'still', u'think', u'film', u'first'])\n",
      "('common elemets %d', 8)\n",
      "com\n",
      "('adding word ', u'movie')\n",
      "('adding word ', u'loose')\n",
      "('adding word ', u'times')\n",
      "('adding word ', u'every')\n",
      "('adding word ', u'characters')\n",
      "('adding word ', u'time')\n",
      "('adding word ', u'nothing')\n",
      "('adding word ', u'film')\n",
      "{u'even': 0, u'made': 1, u'point': 2, u'movie': 3, u'loose': 4, u'scenes': 5, u'times': 12, u'every': 13, u'take': 6, u'characters': 14, u'time': 7, u'nothing': 15, u'nice': 11, u'think': 9, u'film': 10, u'first': 8}\n",
      " [nrows 2, ncols 16, nnz 20]\n",
      "set d--\n",
      "set([u'all', u'abilities', u'enter', u'over', u'atmosphere', u'report', u'afro', u'Ellington', u'Warrne', u'21st', u'mother', u'Oscar', u'character', u',', u'pottier', u'winning', u'to', u'choose', u'actors', u'his', u'Duke', u'Louis', u'watch', u'made', u'every', u'mixing', u'world', u'now', u'him', u'prize', u'success', u'she', u'dramatical', u'Ray', u'be', u'national', u'alive', u'up', u'past', u'choosing', u'anthem', u'Gerogia', u'best', u'for', u'movie', u'decision', u'since', u'state', u'between', u'honorable', u'receiving', u'supporting', u'turning', u'ability', u'corn', u'eternal', u'strong', u'legend', u'on', u'great', u'talent', u'brilliant', u'of', u'times', u's', u'place', u'(', u'adapted', u'.It', u'golden', u'Jamie', u'Sidney', u'point', u'acted', u'two', u'Foxx', u'promise', u'Hollywood', u'.', u'Armstrong', u'from', u'her', u'prove', u'leading', u'century', u'copies', u'by', u'their', u'was', u'until', u'life', u'himself', u'that', u'mind', u'American', u'evidence', u'with', u'glad', u'he', u'sound', u'king', u'grew', u'this', u'work', u'appearance', u'scenes', u'movies', u'those', u'Cole', u'my', u'at', u'and', u'Sharon', u'played', u'am', u'an', u'as', u'Georgia', u'periods', u'Nat', u'in', u'cleverness', u'film', u'ray', u'end', u')', u'song', u'when', u'actor', u'beside', u'till', u'role', u'take', u'grand', u'1893', u'nice', u'eternity', u'independent', u'I', u'who', u'upon', u'cinematic', u'director', u'core', u'icon', u'a', u'succeeded', u'i', u'Charles', u'analysis', u'person', u'so', u'2004', u'time', u'position', u'the', u'accidents', u'fact', u'songs'])\n",
      "features--\n",
      "set([u'even', u'made', u'nice', u'point', u'movie', u'loose', u'scenes', u'times', u'every', u'take', u'characters', u'time', u'nothing', u'still', u'think', u'film', u'first'])\n",
      "('common elemets %d', 10)\n",
      "com\n",
      "('adding word ', u'made')\n",
      "('adding word ', u'point')\n",
      "('adding word ', u'movie')\n",
      "('adding word ', u'scenes')\n",
      "('adding word ', u'times')\n",
      "('adding word ', u'every')\n",
      "('adding word ', u'take')\n",
      "('adding word ', u'time')\n",
      "('adding word ', u'film')\n",
      "('adding word ', u'nice')\n",
      "set d--\n",
      "set([u'What', u'abilities', u'minors', u'show', u'money', u'all', u'controversial', u'Surprise', u'still', u'yet', u'luxuries', u'(', u'had', u',', u'to', u'only', u'Michelle', u'food', u'they', u'naked', u'realistic', u'Paul', u'Elton', u'John', u'even', u'movie', u'got', u'loose', u'come', u'by', u'great', u'figured', u'of', u'1970s', u'think', u'first', u'love', u'one', u'featured', u'names', u'Gilbert', u'noticed', u'two', u'.', u'music', u'films', u'was', u'is', u'happy', u'that', u'about', u'but', u'true', u'me', u'made', u'this', u'critized', u'were', u'more', u'my', u'called', u'and', u'awesome', u'it', u'heard', u'tastes', u'film', u'!', u'no', u')', u'when', u'cinematography', u'recommended', u'becuase', u'I', u'director', u'most', u'characters', u'nothing', u'The', u'friends', u'a', u'Lewis', u'sometimes', u'It', u'so', u'the'])\n",
      "features--\n",
      "set([u'even', u'made', u'nice', u'point', u'movie', u'loose', u'scenes', u'times', u'every', u'take', u'characters', u'time', u'nothing', u'still', u'think', u'film', u'first'])\n",
      "('common elemets %d', 10)\n",
      "com\n",
      "('adding word ', u'even')\n",
      "('adding word ', u'made')\n",
      "('adding word ', u'movie')\n",
      "('adding word ', u'loose')\n",
      "('adding word ', u'characters')\n",
      "('adding word ', u'nothing')\n",
      "('adding word ', u'still')\n",
      "('adding word ', u'think')\n",
      "('adding word ', u'film')\n",
      "('adding word ', u'first')\n",
      "{u'even': 10, u'made': 0, u'point': 1, u'movie': 2, u'loose': 11, u'scenes': 3, u'first': 16, u'times': 4, u'every': 5, u'take': 6, u'characters': 12, u'time': 7, u'nothing': 13, u'still': 14, u'think': 15, u'film': 8, u'nice': 9}\n",
      " [nrows 2, ncols 17, nnz 20]\n"
     ]
    }
   ],
   "source": [
    "docsTrainData = [word_tokenize(l) for l in trainingData['review']]\n",
    "matTrainData  = build_matrix(docsTrainData);\n",
    "csr_info(matTrainData);\n",
    "\n",
    "docsTestData = [word_tokenize(l) for l in testData['review']]\n",
    "matTestData  = build_matrix(docsTestData)\n",
    "csr_info(matTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale and normalize the matrix\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTrainData = csr_idf(matTrainData, copy=True)\n",
    "normalizedTrainData = csr_l2normalize(scaledTrainData, copy=True)\n",
    "# print(\"csr matrix:\", matTrainData.todense(), \"\\n\")\n",
    "# print(\"scaled csr matrix:\", scaledTrainData.todense(), \"\\n\")\n",
    "# print(\"normalized csr matrix:\", normalizedTrainData.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledTestData = csr_idf(matTestData, copy=True)\n",
    "normalizedTestData = csr_l2normalize(scaledTestData, copy=True)\n",
    "# print(\"csr matrix:\", matTestData.todense(), \"\\n\")\n",
    "# print(\"scaled csr matrix:\", scaledTestData.todense(), \"\\n\")\n",
    "# print(\"normalized csr matrix:\", normalizedTestData.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 2, ncols 16, nnz 20]\n",
      " [nrows 2, ncols 17, nnz 20]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 16 while Y.shape[1] == 17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1180-706f8b363e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcsr_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizedTrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcsr_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizedTestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcosineSimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_cosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizedTrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizedTestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosineSimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1180-706f8b363e80>\u001b[0m in \u001b[0;36mcalculate_cosine_sim\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_cosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcosineSimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcosineSimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcsr_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalizedTrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meenakshiparyani/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meenakshiparyani/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    120\u001b[0m         raise ValueError(\"Incompatible dimension for X and Y matrices: \"\n\u001b[1;32m    121\u001b[0m                          \"X.shape[1] == %d while Y.shape[1] == %d\" % (\n\u001b[0;32m--> 122\u001b[0;31m                              X.shape[1], Y.shape[1]))\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 16 while Y.shape[1] == 17"
     ]
    }
   ],
   "source": [
    "# Calculate consine similarity between training and test normalized data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def calculate_cosine_sim(train,test):\n",
    "    cosineSimilarity = cosine_similarity(train,test)\n",
    "    return cosineSimilarity\n",
    "csr_info(normalizedTrainData)\n",
    "csr_info(normalizedTestData)\n",
    "cosineSimilarity = calculate_cosine_sim(normalizedTrainData, normalizedTestData);\n",
    "print(len(cosineSimilarity));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find K nearest neighbors and write to test file\n",
    "f = open('data/test_out.dat.txt', 'w')\n",
    "count = 0\n",
    "for row in cosineSimilarity:\n",
    "    k=1\n",
    "    kLargestSimilarities = np.argpartition(-row, k)\n",
    "    print(kLargestSimilarities)\n",
    "    neighbors = kLargestSimilarities[:k]\n",
    "    print(neighbors)\n",
    "    neighbourReviewClassList = []\n",
    "    neighbourReviewClassNegative = 0\n",
    "    neighbourReviewClassPositive = 0\n",
    "\n",
    "    for review in neighbors:\n",
    "        print('review' , trainingData['rating'][review])\n",
    "        if int(trainingData['rating'][review]) == -1:\n",
    "            neighbourReviewClassNegative+=1\n",
    "        elif int(trainingData['rating'][review]) == 1:\n",
    "            neighbourReviewClassPositive+=1\n",
    "    print(neighbourReviewClassNegative)\n",
    "    print(neighbourReviewClassPositive)\n",
    "    if neighbourReviewClassNegative > neighbourReviewClassPositive:\n",
    "        f.write('-1\\n')\n",
    "        count+=1\n",
    "    else:\n",
    "        f.write('+1\\n')\n",
    "        count+=1\n",
    "\n",
    "print(\"count : \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_out.dat.txt\", \"r\") as fh:\n",
    "    linesOfFormat = fh.readlines()\n",
    "print(len(linesOfFormat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
